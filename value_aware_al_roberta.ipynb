{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKQnO7AirICa"
      },
      "outputs": [],
      "source": [
        "## Mount Drive into Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eblTSysLrWTm"
      },
      "outputs": [],
      "source": [
        "# this notebook has code from https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb\n",
        "!pip install transformers==3.0.2\n",
        "!pip install sklearn\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, confusion_matrix, brier_score_loss\n",
        "import csv\n",
        "import random\n",
        "from random import shuffle\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "kxN9As3BT6XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "LEARNING_RATE = 1e-05\n",
        "EPOCHS = 3\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)\n",
        "torch.manual_seed(3)"
      ],
      "metadata": {
        "id": "7MFhrGcmUk4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofQ7YgCJrmGh"
      },
      "outputs": [],
      "source": [
        "random.seed(41)\n",
        "\n",
        "# define parameters\n",
        "\n",
        "data_folder = '.../usAirline/data/'\n",
        "dataToTrain = 'usAirline_train.csv'\n",
        "dataToVal = 'usAirline_val.csv'\n",
        "dataToTest ='usAirline_test.csv'\n",
        "\n",
        "train_feat = 'X_train.npy'\n",
        "val_feat = 'X_val.npy'\n",
        "test_feat = 'X_test.npy'\n",
        "\n",
        "txt = 'text'\n",
        "goldLabel = 'airline_sentiment'\n",
        "iID = 'itemID'\n",
        "dfColumns = [txt, goldLabel]\n",
        "\n",
        "# AL parameters\n",
        "al_strategies           = ['uncertainty']\n",
        "minimum_training_items  = 3                                                # minimum number of training items before we first train a model\n",
        "alBatchNum              = 88                                                # define the total number of batches in active learning pipeline\n",
        "alBatchSize             = 100                                               # define the size of one batch in active learning pipeline\n",
        "\n",
        "controlList2 = [25, 50, 88]\n",
        "#cost-based parameters\n",
        "Vr = 0.0\n",
        "Vc = 1.0\n",
        "Vw_list = [0, -0.1, -0.2, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9, -1.0, -2.0, -4.0, -8.0, -10.0, -100.0]\n",
        "\n",
        "confT_list = list(np.arange(0, 1.01, 0.01))\n",
        "\n",
        "modelName = 'roberta'\n",
        "datasetName = 'usAirline'\n",
        "\n",
        "res_path = '.../AL/res/usAirline/roberta/' # specify the path to keep results\n",
        "logfile_name = \"{}_{}_rnd41_\".format(datasetName,modelName)   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len, txt, goldLabel):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = self.data[txt]\n",
        "        self.targets = self.data[goldLabel]\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                } "
      ],
      "metadata": {
        "id": "nN3ETP0ZVCOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL5a1ELEueWi"
      },
      "outputs": [],
      "source": [
        "class Data():\n",
        "    \n",
        "    def __init__(self, filename, setN, feat):\n",
        "        \n",
        "        # each dataset will have a pool of data, together with their IDs and gold labels \n",
        "        self.poolData = np.array([])\n",
        "        self.poolGoldLabels = np.array([])\n",
        "        \n",
        "        dt = pd.read_csv(filename)\n",
        "        #dt = dt.dropna()\n",
        "        dt = dt.reset_index(drop=True)\n",
        "        dt['itemID'] = np.arange(dt.shape[0])\n",
        "        y = dt[goldLabel].values\n",
        "        X = np.array(dt[txt].tolist())\n",
        "        \n",
        "        self.data = dt\n",
        "        self.poolDataEmb = X\n",
        "        self.poolGoldLabels = y\n",
        "        self.mClass = list(set(self.poolGoldLabels.tolist()))\n",
        "        \n",
        "    def setStartState(self, nStart):\n",
        "        ''' This functions creates the initial training set which contains the equal number of samples per class\n",
        "        Input:\n",
        "        nStart -- number of labelled datapoints (size of training set)\n",
        "        '''\n",
        "        self.nStart = nStart\n",
        "        data = self.data.copy()\n",
        "        # get predefined points so that all classes are represented and initial classifier could be trained.\n",
        "        sampledIndices = []\n",
        "        for cls in self.mClass:\n",
        "            indices = np.array(np.where(self.poolGoldLabels == cls)).tolist()[0]\n",
        "            idx = random.sample(indices, nStart // len(mClass))\n",
        "            sampledIndices = sampledIndices + idx\n",
        "\n",
        "        sData = data.iloc[sampledIndices]\n",
        "        self.labeledSet = sData.reset_index(drop=True)\n",
        "        droppedData = data.drop(sampledIndices)\n",
        "        self.unlabeledSet = droppedData.reset_index(drop=True)\n",
        "\n",
        "def cost_based_threshold(k):\n",
        "    t = (k)/(k+1)\n",
        "    return t\n",
        "\n",
        "def calculate_value(y_hat_proba, y, t, Vr, Vc, Vw):\n",
        "\n",
        "    y_pred = np.array([np.where(l == np.amax(l))[0][0] if (np.amax(l) > t) else -1 for l in y_hat_proba])\n",
        "\n",
        "    # now lets compute the actual value of each prediction\n",
        "    \n",
        "    value_vector = np.full(y_pred.shape[0], Vc)\n",
        "\n",
        "    value_vector[(y_pred != y) & (y_pred != -1)] = Vw\n",
        "    \n",
        "    #loss due to asking humans\n",
        "    value_vector[y_pred == -1] = Vr\n",
        "    value = np.sum(value_vector) / len(y)\n",
        "\n",
        "    numOfRejectedSamples = np.count_nonzero(y_pred == -1)\n",
        "    numOfWrongPredictions = np.count_nonzero((y_pred != y) & (y_pred != -1))\n",
        "    return value, numOfRejectedSamples, numOfWrongPredictions\n",
        "\n",
        "def find_optimum_confidence_threshold(y_hat_proba, y, t_list, Vr, Vc, Vw):\n",
        "\n",
        "    cost_list = {}\n",
        "\n",
        "    for t in t_list:\n",
        "        # here we define K = fn_c_norm, change it based on task. \n",
        "        value, _ , __ = calculate_value(y_hat_proba, y, t, Vr, Vc, Vw)\n",
        "        cost_list[\"{}\".format(t)] = value\n",
        "    # find t values with maximum value\n",
        "    maxValue = max(cost_list.values())\n",
        "    optTList = [float(k) for k, v in cost_list.items() if v == maxValue]\n",
        "    # pick the one with the lowest confidence\n",
        "    optimumT = min(optTList)\n",
        "\n",
        "    return optimumT, cost_list\n",
        "\n",
        "def uncertainty_sampling(distances, number):\n",
        "    \"\"\"Returns batch of datapoints with smallest margin/highest uncertainty.\n",
        "    For binary classification, can just take the absolute distance to decision\n",
        "    boundary for each point.\n",
        "    For multiclass classification, must consider the margin between distance for\n",
        "    top two most likely classes.\n",
        "    Returns:\n",
        "      indices of points selected to add using margin active learner\n",
        "    \"\"\"\n",
        "    '''Points are sampled according to uncertainty sampling criterion'''\n",
        "\n",
        "  #  distances = model.predict_proba(unl_emb)\n",
        "\n",
        "    if len(distances.shape) < 2:\n",
        "      min_margin = abs(distances)\n",
        "    else:\n",
        "      sort_distances = np.sort(distances, 1)[:, -2:]\n",
        "      min_margin = sort_distances[:, 1] - sort_distances[:, 0]\n",
        "    score_indices = np.argsort(min_margin)\n",
        "    selected_samples = score_indices[0:number]\n",
        "    return selected_samples\n",
        "\n",
        "def certainty_sampling(distances, number):\n",
        "    \"\"\"Returns batch of datapoints with highest margin/smallest uncertainty.\n",
        "    For binary classification, can just take the absolute distance to decision\n",
        "    boundary for each point.\n",
        "    For multiclass classification, must consider the margin between distance for\n",
        "    top two most likely classes.\n",
        "    Returns:\n",
        "      indices of points selected to add using margin active learner\n",
        "    \"\"\"\n",
        "    '''Points are sampled according to certainty sampling criterion'''\n",
        "\n",
        "   # distances = model.predict_proba(unl_emb)\n",
        "\n",
        "    if len(distances.shape) < 2:\n",
        "      min_margin = abs(distances)\n",
        "    else:\n",
        "      sort_distances = np.sort(distances, 1)[:, -2:]\n",
        "      min_margin = sort_distances[:, 1] - sort_distances[:, 0]\n",
        "    score_indices = np.argsort(min_margin)\n",
        "    score_indices_reversed = score_indices[::-1]\n",
        "    selected_samples = score_indices_reversed[0:number]\n",
        "    return selected_samples\n",
        "\n",
        "def random_sampling(dataIds, nQuery):\n",
        "    '''Randomly samples the points'''\n",
        "    query_idx = random.sample(range(len(dataIds)), nQuery)\n",
        "    selectedIndex = dataIds[query_idx]\n",
        "    return selectedIndex\n",
        "\n",
        "def threshold_oriented_sampling(probs, number, t):\n",
        "    #probs = model.predict_proba(unl_emb)\n",
        "    margins = np.array([abs(np.amax(l) - t) for l in probs])\n",
        "    score_indices = np.argsort(margins)\n",
        "    selected_samples = score_indices[0:number]\n",
        "    return selected_samples\n",
        "\n",
        "def tos_below(probs, number, t):\n",
        "    #probs = model.predict_proba(unl_emb)\n",
        "    indices_acc = np.array([1 if np.amax(l) > t else 0 for l in probs])\n",
        "    indices_accepted = np.array(np.where(indices_acc == 1)[0])\n",
        "    margins = np.array([abs(np.amax(l) - t) for l in probs])\n",
        "    score_indices = np.argsort(margins)\n",
        "    score_indices = np.setdiff1d(score_indices, indices_accepted)\n",
        "    selected_samples = score_indices[0:number]\n",
        "    return selected_samples\n",
        "\n",
        "def tos_above(probs, number, t):\n",
        "    #probs = model.predict_proba(unl_emb)\n",
        "    indices_rej = np.array([1 if np.amax(l) <= t else 0 for l in probs])\n",
        "    indices_rejected = np.array(np.where(indices_rej == 1)[0])\n",
        "    margins = np.array([abs(np.amax(l) - t) for l in probs])\n",
        "    score_indices = np.argsort(margins)\n",
        "    score_indices = np.setdiff1d(score_indices, indices_rejected)\n",
        "    selected_samples = score_indices[0:number]\n",
        "    return selected_samples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load datasets\n",
        "pool = Data(data_folder + dataToTrain, 'train', train_feat)\n",
        "mClass =  pool.mClass\n",
        "pool.setStartState(minimum_training_items)\n",
        "\n",
        "validation = Data(data_folder + dataToVal, 'val', val_feat)\n",
        "validation_data = validation.data\n",
        "y_val = np.array(validation_data[goldLabel].tolist())\n",
        "test = Data(data_folder + dataToTest, 'test', test_feat)\n",
        "test_data = test.data\n",
        "y_test = np.array(test_data[goldLabel].tolist())\n",
        "\n",
        "trainD = pd.read_csv(data_folder + dataToTrain)\n",
        "trainD = trainD[[txt, goldLabel]]\n",
        "valD = pd.read_csv(data_folder + dataToVal)\n",
        "valD = valD[[txt, goldLabel]]\n",
        "validation_set = SentimentData(valD, tokenizer, MAX_LEN, txt, goldLabel)\n",
        "validation_loader = DataLoader(validation_set, **test_params)\n",
        "\n",
        "testD = pd.read_csv(data_folder + dataToTest)\n",
        "testD = testD[[txt, goldLabel]]\n",
        "testing_set = SentimentData(testD, tokenizer, MAX_LEN, txt, goldLabel)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "nAzPUyDTbyuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RobertaClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaClass, self).__init__()\n",
        "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        torch.manual_seed(3)\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        #self.dropout = torch.nn.Dropout(0.3)\n",
        "        torch.manual_seed(3)\n",
        "        self.classifier = torch.nn.Linear(768, len(mClass))\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        #pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "def calcuate_accuracy(preds, targets):\n",
        "    n_correct = (preds==targets).sum().item()\n",
        "    return n_correct"
      ],
      "metadata": {
        "id": "M978zAJGz0Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model\n",
        "model = RobertaClass()\n",
        "model.to(device)\n",
        "\n",
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "1OPhvOJ2z0uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "def train(epoch, model, train_loader, optim):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(train_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _%5000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optim.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return\n",
        "\n",
        "def valid(model, test_loader):\n",
        "    model.eval()\n",
        "    # Tracking variables \n",
        "    logitsList , true_labels = np.array([]), np.array([])\n",
        "    index = 0\n",
        "    # Predict \n",
        "    for _, data in tqdm(enumerate(test_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "        outputs = model(ids, mask, token_type_ids).squeeze()\n",
        "        # Move logits and labels to CPU\n",
        "        outputs = outputs.detach().cpu().numpy()\n",
        "        if outputs.ndim == 1:\n",
        "            probs = np.array([F.softmax(torch.tensor(outputs)).detach().cpu().numpy()])\n",
        "        else:\n",
        "            probs = np.array([F.softmax(torch.tensor(output)).detach().cpu().numpy() for output in outputs])\n",
        "        targets = targets.to('cpu').numpy()\n",
        "        # Store predictions and true labels\n",
        "        if index == 0:\n",
        "            logitsList = probs\n",
        "            true_labels = targets\n",
        "        else:\n",
        "            logitsList = np.concatenate((logitsList, probs), axis=0)\n",
        "            true_labels = np.concatenate((true_labels, targets), axis=0)\n",
        "        index = index + 1\n",
        "    return logitsList, true_labels"
      ],
      "metadata": {
        "id": "d2lPVMrDbzpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR0cRzGpvBMc"
      },
      "outputs": [],
      "source": [
        "for al_strategy in al_strategies:\n",
        "\n",
        "    poolData = pool.data.copy()\n",
        "    training_data = pool.labeledSet.copy()\n",
        "    unlabeled_data = pool.unlabeledSet.copy()\n",
        "    batchSize = alBatchSize\n",
        "\n",
        "    poolDataEmb_val = validation.poolDataEmb\n",
        "    poolDataEmb_test = test.poolDataEmb\n",
        "    train_data = pool.poolDataEmb[poolData.index[poolData[iID].isin(training_data[iID].values)].tolist()]\n",
        "    train_labels = np.array(training_data[goldLabel].tolist())\n",
        "\n",
        "    #Start active learning\n",
        "    sampleIds = []\n",
        "    samplingRanks = []\n",
        "    samplesDict = {}\n",
        "    samplesDict[0] = training_data[iID].tolist()\n",
        "\n",
        "    rv_path = res_path + logfile_name + al_strategy + \"_value.csv\"\n",
        "    with open(rv_path, 'w') as f:\n",
        "        c = 'batch, Vr, Vc, Vw, k, t_cal, t_opt_val, t_opt_train, t_opt_test, value_test, rej_test, wrong_test, value_train, rej_train, wrong_train, value_test_opt, rej_test_opt, wrong_test_opt, value_train_opt, rej_train_opt, wrong_train_opt, value_test_opt_test, rej_test_opt_test, wrong_test_opt_test'\n",
        "        f.write(c + '\\n')\n",
        "\n",
        "    training_set = SentimentData(pd.DataFrame(list(zip(train_data.tolist(), train_labels)), columns = dfColumns), tokenizer, MAX_LEN, txt, goldLabel)\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    \n",
        "    #fine-tune the model\n",
        "    for epoch in range(EPOCHS):\n",
        "        train(epoch, model, training_loader, optimizer)\n",
        "\n",
        "    trainingTest_loader = DataLoader(training_set, **test_params)\n",
        "    logits_train, train_labels = valid(model, trainingTest_loader)\n",
        "    logits_val, y_val = valid(model, validation_loader)\n",
        "    logits_test, y_test = valid(model, testing_loader)\n",
        "\n",
        "    for Vw in Vw_list:\n",
        "        k = (-1)*(Vw / Vc)\n",
        "        t = cost_based_threshold(k)\n",
        "        value_test, rej_test, wrong_test = calculate_value(logits_test, y_test, t, Vr, Vc, Vw)\n",
        "        value_train, rej_train, wrong_train = calculate_value(logits_train, train_labels, t, Vr, Vc, Vw)\n",
        "\n",
        "        t_opt, cost_list = find_optimum_confidence_threshold(logits_val, y_val, confT_list, Vr, Vc, Vw)\n",
        "        value_test_opt, rej_test_opt, wrong_test_opt = calculate_value(logits_test, y_test, t_opt, Vr, Vc, Vw)\n",
        "        value_train_opt, rej_train_opt, wrong_train_opt  = calculate_value(logits_train, train_labels, t_opt, Vr, Vc, Vw)\n",
        "\n",
        "        t_opt_train, cost_list_ = find_optimum_confidence_threshold(logits_train, train_labels, confT_list, Vr, Vc, Vw)\n",
        "        t_opt_test, cost_list_ = find_optimum_confidence_threshold(logits_test, y_test, confT_list, Vr, Vc, Vw)\n",
        "        value_test_opt_test, rej_test_opt_test, wrong_test_opt_test = calculate_value(logits_test, y_test, t_opt_test, Vr, Vc, Vw)\n",
        "\n",
        "        with open(rv_path, 'a') as f:\n",
        "            res_i = '{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(0, Vr, Vc, Vw, k, t, t_opt, t_opt_train, t_opt_test, value_test, rej_test, wrong_test, value_train, rej_train, wrong_train, value_test_opt, rej_test_opt, wrong_test_opt, value_train_opt, rej_train_opt, wrong_train_opt, value_test_opt_test, rej_test_opt_test, wrong_test_opt_test)\n",
        "            f.write(res_i)\n",
        "    \n",
        "    for alBatch in range(1, alBatchNum + 1, 1):\n",
        "        sampledIndices = []\n",
        "\n",
        "        unl_dataEmb = pool.poolDataEmb[poolData.index[poolData[iID].isin(unlabeled_data[iID].values)].tolist()]\n",
        "        unl_dataLabels = pool.poolGoldLabels[poolData.index[poolData[iID].isin(unlabeled_data[iID].values)].tolist()]\n",
        "        unlabeled_set = SentimentData(pd.DataFrame(list(zip(unl_dataEmb.tolist(), unl_dataLabels.tolist())), columns = dfColumns), tokenizer, MAX_LEN, txt, goldLabel)\n",
        "        unlabeled_loader = DataLoader(unlabeled_set, **test_params)\n",
        "        probs_unlabeled, labels_unlabeled = valid(model, unlabeled_loader)\n",
        "\n",
        "        if alBatch == alBatchNum:\n",
        "            batchSize = len(unlabeled_data[iID].values)\n",
        "            print(\"alBatchSize changed to: \", batchSize)\n",
        "\n",
        "        if al_strategy == 'uncertainty':\n",
        "            idx = uncertainty_sampling(probs_unlabeled, batchSize)\n",
        "            sampledIndices = unlabeled_data.loc[idx][iID].tolist()\n",
        "            for i in sampledIndices: sampleIds.append(i)\n",
        "        elif al_strategy == 'certainty':\n",
        "            idx = certainty_sampling(probs_unlabeled, batchSize)\n",
        "            sampledIndices = unlabeled_data.loc[idx][iID].tolist()\n",
        "            for i in sampledIndices: sampleIds.append(i)\n",
        "        elif al_strategy == 'random':\n",
        "            sampledIndices = random_sampling(unlabeled_data[iID].values, batchSize)\n",
        "            for i in sampledIndices: sampleIds.append(i)\n",
        "        else:\n",
        "            #default sampling, random\n",
        "            sampledIndices = random_sampling(unlabeled_data[iID].values, batchSize)\n",
        "            for i in sampledIndices: sampleIds.append(i)\n",
        "\n",
        "        sampledSet = poolData.loc[poolData[iID].isin(sampledIndices)]\n",
        "        samplesDict[alBatch] = sampledIndices\n",
        "                    \n",
        "        training_data.reset_index(drop=True)\n",
        "        sampledSet.reset_index(drop=True)\n",
        "        training_data = pd.concat([training_data, sampledSet], axis=0).reset_index(drop=True)\n",
        "        training_data = training_data.sort_values(iID)\n",
        "        indices = unlabeled_data.loc[unlabeled_data[iID].isin(sampledIndices)].index.to_list()\n",
        "        unlabeled_data = unlabeled_data.drop(indices).reset_index(drop=True)\n",
        "        unlabeled_data = unlabeled_data.reset_index(drop=True)\n",
        "  \n",
        "        train_data = pool.poolDataEmb[poolData.index[poolData[iID].isin(training_data[iID].values)].tolist()]\n",
        "        train_labels = np.array(training_data[goldLabel].tolist())\n",
        "\n",
        "        training_set = SentimentData(pd.DataFrame(list(zip(train_data.tolist(), train_labels)), columns = dfColumns), tokenizer, MAX_LEN, txt, goldLabel)\n",
        "        training_loader = DataLoader(training_set, **train_params)\n",
        "    \n",
        "        model = RobertaClass()\n",
        "        model.to(device)\n",
        "        optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        #fine-tune the model\n",
        "        for epoch in range(EPOCHS):\n",
        "            train(epoch, model, training_loader, optimizer)\n",
        "\n",
        "        trainingTest_loader = DataLoader(training_set, **test_params)\n",
        "        logits_train, train_labels = valid(model, trainingTest_loader)\n",
        "        logits_val, y_val = valid(model, validation_loader)\n",
        "        logits_test, y_test = valid(model, testing_loader)\n",
        "\n",
        "        if alBatch in controlList2:\n",
        "            col = []\n",
        "            for i in range(logits_test.shape[1]):\n",
        "                col.append(str(i))\n",
        "            df_lgt = pd.DataFrame(logits_test, columns = col)\n",
        "            df_lgt['y'] = y_test\n",
        "            df_lgt.to_csv(res_path + logfile_name + al_strategy + '_b_{}_logits.csv'.format(alBatch))\n",
        "\n",
        "        for Vw in Vw_list:\n",
        "            k = (-1)*(Vw / Vc)\n",
        "            t = cost_based_threshold(k)\n",
        "\n",
        "            value_test, rej_test, wrong_test = calculate_value(logits_test, y_test, t, Vr, Vc, Vw)\n",
        "            value_train, rej_train, wrong_train = calculate_value(logits_train, train_labels,  t, Vr, Vc, Vw)\n",
        "\n",
        "            t_opt, cost_list = find_optimum_confidence_threshold(logits_val, y_val, confT_list, Vr, Vc, Vw)\n",
        "\n",
        "            value_test_opt, rej_test_opt, wrong_test_opt = calculate_value(logits_test, y_test, t_opt, Vr, Vc, Vw)\n",
        "            value_train_opt, rej_train_opt, wrong_train_opt  = calculate_value(logits_train, train_labels, t_opt, Vr, Vc, Vw)\n",
        "\n",
        "            t_opt_train, cost_list_ = find_optimum_confidence_threshold(logits_train, train_labels, confT_list, Vr, Vc, Vw)\n",
        "            t_opt_test, cost_list_ = find_optimum_confidence_threshold(logits_test, y_test, confT_list, Vr, Vc, Vw)\n",
        "            value_test_opt_test, rej_test_opt_test, wrong_test_opt_test = calculate_value(logits_test, y_test, t_opt_test, Vr, Vc, Vw)\n",
        "\n",
        "            with open(rv_path, 'a') as f:\n",
        "                res_i = '{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(alBatch, Vr, Vc, Vw, k, t, t_opt, t_opt_train, t_opt_test, value_test, rej_test, wrong_test, value_train, rej_train, wrong_train, value_test_opt, rej_test_opt, wrong_test_opt, value_train_opt, rej_train_opt, wrong_train_opt, value_test_opt_test, rej_test_opt_test, wrong_test_opt_test)\n",
        "                f.write(res_i) \n",
        "        \n",
        "  #  training_data.to_csv(res_path + logfile_name + al_strategy + \"_trainingData.csv\")\n",
        "    samplesDict_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in samplesDict.items() ]))\n",
        "    samplesDict_df.to_csv(res_path + logfile_name + al_strategy + \"_sampledItems.csv\")\n",
        "\n",
        "    output_model_file = res_path + logfile_name + al_strategy + '_roberta_sentiment.bin'\n",
        "    output_vocab_file = './'\n",
        "\n",
        "    model_to_save = model\n",
        "    torch.save(model_to_save, output_model_file)\n",
        "    tokenizer.save_vocabulary(output_vocab_file)\n",
        "    print('All files saved')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "value-aware_al_roberta.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}